<!DOCTYPE html>
<html lang="en">

<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css">
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.3/dist/jquery.slim.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.3.0/css/all.min.css">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js"></script>
    <script src="https://cdn.zingchart.com/zingchart.min.js"></script>
    <title>Tamoghnas's Diffusion MRI Paper</title>
    <link href="blog.css" rel="stylesheet" type="text/css" />

    <!-- load the d3.js library -->
    <!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.17/d3.min.js"></script> -->

    <!-- blinking arrow -->
    <!-- <div class="arrow-container">
        <div class="arrow"></div>
    </div> -->



</head>

<body>
    <nav class="navbar navbar-expand-md header" style="background-color: black;">
        <a class="navbar-brand" href="#" style="color: #b8860b; font-size: 28px; font-weight: bold;">IGC</a>

        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav"
            aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <div class="navbar navbar-title">
            <h1>Vision Transformers for AD</h1>
        </div>

        <div class="collapse navbar-collapse" id="navbarNav">
            <ul class="navbar-nav ml-auto">
                <li class="nav-item active">
                    <a class="nav-link" style="color: whitesmoke;" href="index.html">Home</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" style="color: whitesmoke;" href="../AI@IGC.html">AI @ IGC</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link hover-link" href="blogs.html">Blog</a>
                </li>
            </ul>
        </div>

    </nav>

    <div id="layout">
        <div id="sidebar">
            <h3>Jump to</h3>
            <div id="custom-toc-container">
                <div class="toc">
                    <ul>
                        <li>
                        <li><a href="#vit_title">Vision Transformers</a>
                            <ul>
                                <li><a href="#model_details">Model Details</a></li>
                                <li><a href="#intended_use">Intended Use</a></li>
                                <li><a href="#metrics">Metrics</a></li>
                                <li><a href="#training_data">Training Data</a></li>
                                <li><a href="#evalution_data">Evaluation Data</a></li>
                                <li><a href="#caveats_recommendations">Caveats and Recommendations</a></li>
                                <li><a href="#quantative_analysis">Quantitative Analysis</a></li>
                                <li><a href="#key_takeaways">Key Takeaway</a></li>
                            </ul>



                </div>

            </div>
        </div>
        <div id="content-container">
            <div class="markdown-body markdown-content">
                <!-- <div class="container text-center" id="vit"> -->
                <h2 style="color : #D4AC0D" id="vit_title"><b>Efficiently Training Vision Transformers on
                        Structural MRI Scans for
                        Alzheimer’s Disease Detection</b></h2>
                <!-- <p class="container pt-3 pr-5 pl-5 text-justify" style="color : white"> One of the most common deep
                        learning
                        layers for processing brain MRI inputs are the 3D convolutional layers. 3D convolutional neural
                        networks
                        - also known as roCNNs, or ConvNets - are an extension of 2D CNNs where the kernels are 3D. They
                        were
                        developed in order to take advantage of the spatial information present in 3D medical images.
                        However,
                        3D convolutional neural networks require more parameters and can only work with are specific to 3D
                        images. This specificity restricts the use or adapt of any typically neural network trained on more
                        common 2D images (or natural images, such as photographs), or to adapt such a network to
                        neuroimaging
                        tasks. To this end, we introduced 2D-Slice CNN models that process one 2D slice at a time with
                        traditional 2D convolutional networks and combine the resulting representations using recurrent
                        operations and set operations. These models achieve performance similar to 3D-CNNs while being
                        faster to
                        train, more data-efficient, and less susceptible to noise in MRI slices. We further showed in a
                        paper at
                        ISBI 2023 that these 2D encoder-based models improve performance when initialized with
                        state-of-the-art
                        standard computer vision models.</p> -->

                <h3 class="container" id="model_details">Model Details</h3>
                <ul>
                    <li>The vision transformer (ViT) class of architectures were trained and tested for the AD
                        diagnosis
                        task based on brain MRI.
                    </li>
                    <li>Two main variants of the ViTs included in this paper were the ViT B/16 and the
                        scaled-down
                        version
                        called NiT.</li>
                    <li>Two main variants of the ViTs included in this paper were the ViT B/16 and the scaled
                        down version
                        called NiT.
                    </li>
                    <li>We show the potential of pretraining on synthetic MRI scans for neuroimaging tasks.
                    </li>
                    <li>We present the effect of advanced training strategies like learning rate decay with
                        warmups, mixup
                        data augmentation.
                    </li>
                    <li>Data-scaling curves show the effect of training sample size on test-time performance of
                        the Vision
                        Transformer.
                    </li>
                </ul>
                <img class="container pt-3 pr-5 pl-5" src="../blogimages/vit_archi.png">
                <h3 class="container" id="intended_use">Intended Use</h3>
                <ul>
                    <li>Intended to be used for a wide range of neuroimaging tasks.</li>
                    <li>Potential to match and outperform CNN architectures with the use of the appropriate
                        architecture
                        and
                        pre-training/fine-tuning strategies.</li>
                    <li>Not tested on tasks other than AD diagnosis using T1-weighted brain MRI.</li>
                </ul>

                <h3 class="container" id="metrics">Metrics</h3>
                <ul>
                    <li>Area under the receiver characteristic curve (ROC-AUC).
                    </li>
                    <li>Performance metrics based on the ROC curve including accuracy, precision, recall,
                        F1-score.
                    </li>
                    <li>Optional use of Youden’s index to optimize the threshold selection from the ROC curve.
                    </li>
                </ul>


                <h3 class="container">Training Data</h3>
                <ul>
                    <li>Trained on 2,577 (from 747 subjects) T1-weighted brain MRI scans from Alzheimer’s
                        Disease
                        Neuroimaging Initiative (ADNI). Additional details in the reference below.
                    </li>
                    <li>38,710 T1-w scans from the UK Biobank and 100,000 from the LDM100K dataset were used for
                        pre-training.
                    </li>
                </ul>

                <h3 class="container" id="evaluation_data">Evaluation Data</h3>
                <ul>
                    <li>Models tested on a test set with 1,219 T1-w scans (from 359 subjects) from ADNI.
                    </li>
                    <li>600 scans (600 subjects) from Open Access Series of Imaging Studies, phase 3 (OASIS)
                        used as an
                        out-of-distribution dataset to independently validate generalization capability.
                    </li>
                </ul>

                <h3 class="container" id="caveats_recommendations">Caveats and Recommendations</h3>
                <ul>
                    <li>While the hyperparameters and training strategies in the paper were tested for the
                        experiments
                        proposed in the paper, additional tuning might be required given different
                        architectures, tasks, and
                        datasets.
                    </li>
                </ul>

                <h3 class="container" id="quantitative_analysis">Quantitative Analysis</h3>
                <img class="container pt-3 pr-5 pl-5" src="../blogimages/vit_table.png">

                <h3 class="container" id="key_takeaway">Key Takeaway</h3>
                <ul>
                    <li> <b>New vision transformer</b> class of architectures for AD diagnosis using T1-weighted
                        MRI was
                        tested to boost performance by 1% from scratch compared to CNN; and 9% and 5% using
                        pre-training on
                        real and synthetic data, respectively.</li>
                </ul>


            </div>
        </div>
    </div>
    <div id="all_posts">
        <h3> All Posts</h3>
        <div id="all-posts-container">
            <ul class='posts'>
                <li><a href='introducing.html'>Introducing AI for Neuroimaging @ IGC</a>
                <li><a href='dMRI_dementia.html'>Tamoghnas's Diffusion MRI Paper</a>
                <li><a href='sipaim_2022_nikhil_transfer.html'>Nikhil's SIPAIM 2022 Transfer Learning Paper</a>
                <li><a href='table+t1mri_amyloid.html'>Features + T1 MRI for amyloid prediction</a>
                <li><a href='harmonizing_with_cycle_gan.html'>Dheeraj's Harmonization Paper</a>
                    <li><a href='vit_AD.html'>Nikhil's Vision Transformers</a>
            </ul>
        </div>
    </div>
    </div>
</body>

<script>

</script>

<script type="text/javascript" src="active-view.js"></script>

</html>