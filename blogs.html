<!DOCTYPE html>
<html lang="en">

<head>
    <title>IGC USC</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css">
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.3/dist/jquery.slim.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.3.0/css/all.min.css">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js"></script>
    <script src="https://cdn.zingchart.com/zingchart.min.js"></script>
    <link rel="stylesheet" href="style.css">
   


    <!-- load the d3.js library -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.17/d3.min.js"></script>



    <style>
        #chartdiv {
            width: 100%;
            height: 550px;
            max-width: 100%;
        }
    </style>

    <!-- blinking arrow -->
  <div class="arrow-container">
    <div class="arrow"></div>
  </div>



</head>

<body style="background-color: black;">


    <nav class="navbar navbar-expand-md header" style="background-color: black;">

        <a class="navbar-brand" href="#" style="color: #b8860b; font-size: 28px; font-weight: bold;">IGC</a>

        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav"
            aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse" id="navbarNav">
            <ul class="navbar-nav ml-auto">
                <li class="nav-item active">
                    <a class="nav-link" style="color: whitesmoke;" href="index.html">Home</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" style="color: whitesmoke;" href="AI@IGC.html">AI @ IGC</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link hover-link" href="blogs.html">Blog</a>
                </li>
            </ul>
        </div>

    </nav>

    <br>
    <br>
    <br>
    <br>



    <div class="container text-center">
        <div class="jombotron">
            <!-- <h6 style="color: #D4AC0D; font-weight: bold;" >IGC's</h6> -->
            <h2 style="color : #D4AC0D"><b>Introducing AI for Neuroimaging @ IGC</b></h2>
            <p class="container pt-2 pr-5 pl-5 text-justify" style="color : white"> We are the Imaging Genetics
                Center
                AI Team from the Stevens Institute for Neuroimaging and Informatics (INI), at USC’s Keck School of
                Medicine. We develop advanced “big data” methods - including a variety of AI methods - to study the
                human brain. We study over 20 major brain diseases, including Alzheimer’s disease and Parkinson’s
                disease, disorders of brain development such as autism, psychiatric illnesses such as bipolar
                disorder,
                and neurogenetic disorders. A part of our research, weefforts is dedicated to are developing and
                adaopting modern deep-learning algorithms to model brain data acquired by various brain imaging
                techniques, such as mMagnetic rResonance iImaging (MRI) and dDiffusion tTensor iImaging (DTIs). We
                have
                proposed many novelnew model architectures to process MRI inputs, for various tasks such as
                computer-aided diagnosis or prognosisrnoi. Addressing challenges specific to neuroimaging, we have
                demonstrated the benefits of using transfer learning and pre-training for neuroimaging, especially
                as
                most of the neuroimaging datasets are small (usually hundreds rather than tens of thousands of
                scans)o .
                We have also evaluated the harmonization of datasets andor the additional value of fusing multiple
                modalities in neuroimaging studies. This post summarizes our recent efforts in these directions.
            </p>

        </div>
    </div>
    <br>
    <br>
    <div class="container text-center">
        <div>
            <h2 style="color: #D4AC0D"><b>Blogs</b></h2>
        </div>
    </div>
    <br>
    <div class="container">
        <div class="table-responsive">
            <table class="table table-dark table-bordered table-striped text-center">

                <thead>
                    <tr>
                        <th scope="col">#</th>
                        <th scope="col">Title</th>
                        <th scope="col">Blog</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <th scope="row">1</th>
                        <td>Convolutional Neural Networks (CNNs) for Brain MRI and the use of ImageNet</td>
                        <td><a href="#cnn_for_brain_mri">CNN for Brain MRI</a>
                    </tr>
                    <tr>
                        <th scope="row">2</th>
                        <td>Efficiently Training Vision Transformers on Structural MRI Scans for Alzheimer’s Disease
                            Detection</td>
                        <td><a href="#vit">VIT</a>
                    </tr>



                </tbody>
            </table>
        </div>
    </div>

    <br>
    <hr class="container" color="white">
    <br>

    <div class="container_1" id="cnn_for_brain_mri"></div>
    <div class="container text-center">
        <div class="jombotron">
            <!-- <h6 style="color: #D4AC0D; font-weight: bold;" >IGC's</h6> -->
            <h2 style="color : #D4AC0D"><b>Convolutional Neural Networks (CNNs) for Brain MRI and the use of
                    ImageNet</b></h2>
            <p class="container pt-2 pr-5 pl-5 text-justify" style="color : white"> One of the most common deep
                learning
                layers for processing brain MRI inputs areis the 3D convolutional layers., 3D convolutional neural
                networks - also known as roCNNs, or ConvNets -
                - are an extension of 2D CNNs where the kernels are 3D. , and tThey were developed in order to take
                advantage of the spatial information present in 3D medical images. However, 3D convolutional neural
                networks require more parameters and can only work with are specific to 3D images. This specificity
                makes it difficult torestricts the use or adapt of any a typically neural network trained on more
                common
                2D images (or natural images, such as photographs), or to adapt such a networkbe transferred to
                neuroimaging tasks. To this end, we introduced 2D-Slice CNN models that process one 2D slice at a
                time
                with traditional 2D convolutional networks and combine the resulting representations using recurrent
                operations and set operations. These models achieve performance similar to 3D-CNNs while being
                faster to
                train, more data-efficient, and less susceptible to noise in MRI slices. We further showed in a
                paper at
                ISBI 2023 that these 2D encoder-based models improve performance when initialized with
                state-of-the-artstandard computer vision models.
            </p>

        </div>
    </div>

    <br>
    <hr class="container" color="white">
    <br>

    <div class="container_1" id="vit"></div>
    <div class="container text-center" id="vit">
        <div class="jombotron">
            <h2 style="color : #D4AC0D"><b>Efficiently Training Vision Transformers on Structural MRI Scans for
                    Alzheimer’s Disease Detection</b></h2>
            <p class="container pt-3 pr-5 pl-5 text-justify" style="color : white"> One of the most common deep
                learning
                layers for processing brain MRI inputs are the 3D convolutional layers. 3D convolutional neural
                networks
                - also known as roCNNs, or ConvNets - are an extension of 2D CNNs where the kernels are 3D. They
                were
                developed in order to take advantage of the spatial information present in 3D medical images.
                However,
                3D convolutional neural networks require more parameters and can only work with are specific to 3D
                images. This specificity restricts the use or adapt of any typically neural network trained on more
                common 2D images (or natural images, such as photographs), or to adapt such a network to
                neuroimaging
                tasks. To this end, we introduced 2D-Slice CNN models that process one 2D slice at a time with
                traditional 2D convolutional networks and combine the resulting representations using recurrent
                operations and set operations. These models achieve performance similar to 3D-CNNs while being
                faster to
                train, more data-efficient, and less susceptible to noise in MRI slices. We further showed in a
                paper at
                ISBI 2023 that these 2D encoder-based models improve performance when initialized with
                state-of-the-art
                standard computer vision models.</p>

            <h3 class="container pt-3 pr-5 pl-5">Model Details</h3>
            <ul>
                <li>The vision transformer (ViT) class of architectures were trained and tested for the AD diagnosis
                    task based on brain MRI.</li>
                <li>Two main variants of the ViTs included in this paper were the ViT B/16 and the scaled-down
                    version
                    called NiT.</li>
                <li>We show the potential of pretraining on synthetic MRI scans for neuroimaging tasks.</li>
                <li>We present the effect of advanced training strategies like learning rate decay with warmups,
                    Mixup
                    data augmentation.</li>
                <li>Data-scaling curves show the effect of training sample size on test-time performance of the
                    Vision
                    Transformer.</li>
            </ul>
            <img class="container pt-3 pr-5 pl-5" src="blogimages/vit_archi.png">

            <h3 class="container pt-3 pr-5 pl-5">Intended Use</h3>
            <ul>
                <li>Intended to be used for a wide range of neuroimaging tasks.</li>
                <li>Potential to match and outperform CNN architectures with the use of the appropriate architecture
                    and
                    pre-training/fine-tuning strategies.</li>
                <li>Not tested on tasks other than AD diagnosis using T1-weighted brain MRI.</li>
            </ul>

            <h3 class="container pt-3 pr-5 pl-5">Training Data</h3>
            <ul>
                <li>Trained on 2,577 (from 747 subjects) T1-weighted brain MRI scans from Alzheimer’s Disease
                    Neuroimaging Initiative (ADNI). </li>
                <li>Additional details in the reference below.
                    38,710 T1-w scans from the UK Biobank and 100,000 from the LDM100K dataset wereare used for
                    pre-training.</li>
            </ul>

            <h3 class="container pt-3 pr-5 pl-5">Evaluation Data</h3>
            <ul>
                <li>Models tested on a test set with 1,219 T1-w scans (from 359 subjects) from ADNI. </li>
                <li>600 scans (600 subjects) from Open Access Series of Imaging Studies, phase 3 (OASIS) used as an
                    out-of-distribution dataset to independently validate generalization capability.</li>
            </ul>

            <h3 class="container pt-3 pr-5 pl-5">Caveats and Recommendations</h3>
            <ul>
                <li>While the hyperparameters and training strategies in the paper were tested for the experiments
                    proposed in the paper, additional tuning might be required given different architectures, tasks,
                    and
                    datasets.</li>
            </ul>

            <h3 class="container pt-3 pr-5 pl-5">Quantitative Analysis</h3>
            <img class="container pt-3 pr-5 pl-5" src="blogimages/vit_table.png">

            <h3 class="container pt-3 pr-5 pl-5">Key Takeaway</h3>
            <ul>
                <li>New vision transformer class of architectures for AD diagnosis using T1-weighted MRI was tested
                    to
                    boost performance by 1% from scratch compared to CNN; and 9% and 5% using pre-training on real,
                    and
                    synthetic data, respectively.</li>
            </ul>


        </div>
    </div>

    <script src="scripts/papers.js"></script>
</body>

</html>