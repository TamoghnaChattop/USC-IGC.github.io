<!DOCTYPE html>
<html lang="en">
<head>
  <title>Bootstrap Example</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css">
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.3/dist/jquery.slim.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.3.0/css/all.min.css">

  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js"></script>
  
  <link rel="stylesheet" href="style.css">

</head>
<body style="background-color: black;">

    <nav class="navbar navbar-expand-md" style = "background-color: black;">
      
      <a class="navbar-brand" href="#" style="color: #b8860b; font-size: 28px; font-weight: bold;">IGC</a>
      
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      
      <div class="collapse navbar-collapse" id="navbarNav">
          <ul class="navbar-nav ml-auto">
            <li class="nav-item active">
              <a class="nav-link" style = "color: whitesmoke;" href="#">Home</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" style = "color: whitesmoke;" href="#">Datasets</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" style = "color: whitesmoke;" href="#">Architecture</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" style = "color: whitesmoke;" href="#">Publications</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" style = "color: whitesmoke;" href="#">About</a>
            </li>
          </ul>
      </div>
    
    </nav>

    <div class="container-fluid">
      <div class="row">
          <div class="col-md-6">
            <div class="container-fluid bg-cover" style="background-color: black;">
            </div>
          </div>
          
          <div class="col-md-6">
            <div class="container-fluid d-flex h-100">
              <div class="row justify-content-center align-self-center">

                <div class="text-center">
                  <img src = "./assets/IGCLogo.svg" alt="My Happy SVG"/>
                  <hr color="white">

                  <h1 style="color: #D4AC0D; font-weight: bold;" >Imaging Genetics Center</h1>
                  <h5 style="color:  white;"> University of Southern California</h5>
                  <h7 style="color:  gray;"> Marina Del Ray, CA</h7>

                </div>
              </div>
            </div>
          </div>

      </div>
    </div>

    <hr class = "container" color="white">
    <br>


    <div class = "container text-center"> 
      <div class="jombotron">
        <h6 style="color: #D4AC0D; font-weight: bold;" >IGC's</h6>
        <h2 style="color : white"><b>AI & Machine Learning Dashboard</b></h2>
        <p class="container pt-2 pr-5 pl-5" style="color : white"> Welcome to our neuroimaging lab! We specialize in using deep learning techniques to analyze and predict neurological conditions such as Alzheimer's, Parkinson's, Bipolar Disorder, Major Depressive Disorder, PTSD and Autism. Our team of experts also focuses on solving brain analysis problems such as predicting brain age and sex with the help of deep learning based computer vision architectures. Our research and development aim to contribute towards the advancement of the field of neuroimaging, leading to better diagnosis and treatment options for patients.</p>
        
      </div>
    </div>
    <br>

    <hr class = "container" color="white">
    <br>

    <div class="container text-center">
      <div>
        <h2 style="color : white"><b>Datasets</b></h2>
        <br>
        <p style="color : white"><b style="font-size: 20px;">Pre proceesing: </b> All 3D T1-weighted MRI brain scans are pre-processed using a sequence of steps 
          detailed in <a href="https://arxiv.org/pdf/2011.09115.pdf">Pradeep et .al</a>.
          Scans are processed through a standard processing pipeline consisting of reorientation 
          using <a href = "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5770339/"> Fidel et al.</a>, 
          skull stripping using HDBet [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6865732/"> 
            Fabian et al.</a>], nonparametric intensity normalization for bias field correction
            using N4BiasCorrection [<a href="https://pubmed.ncbi.nlm.nih.gov/20378467/">Nicholas et al.</a>], 
            and 6-dof linear registration to MNI152 space using <a href = "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5770339/"> 
            FSL </a>FSL. Scans are subsequently down-sampled to 2-mm resolution using ANTS. The final dimensions of each scan are 91x109x109.
      </div>
    </div>
    <br>
    <h4 style="color : white; text-align: center;"><b>List of Datasets for Deep learning Pipeline</b></h4>
    <br>
    <div class="container">
        <div class="table-responsive">
            <table class="table table-dark table-bordered table-striped text-center">

              <thead>
                <tr>
                  <th scope="col">#</th>
                  <th scope="col">Name</th>
                  <th scope="col">No. of Scans</th>
                  <th scope="col">Domain</th>
                  <th scope="col">Age Range</th>
                  <th scope="col">Reference</th>
                </tr>
              </thead>
              <tbody>
                  <tr>
                    <th scope="row">1</th>
                    <td>UK Bio Bank</td>
                    <td>49,000</td>
                    <td>Controls</td>
                    <td>44.6 - 80.8</td>
                    <td><a href="https://www.ukbiobank.ac.uk/">UKBioBank</a></td>
                  </tr>
                  <tr>
                    <th scope="row">2</th>
                    <td>ADNI</td>
                    <td>2,375</td>
                    <td>Alzheimer, MCI </td>
                    <td>55.1 - 97.4</td>
                    <td><a href="http://adni.loni.usc.edu/">ADNI</a></td>
                  </tr>
                  <tr>
                    <th scope="row">3</th>
                    <td>WHIMS</td>
                    <td>942</td>
                    <td>Controls, MCI, Bio. Sex - Female</td>
                    <td>65 - 80</td>
                    <td><a href="https://sp.whi.org/researchers/data/Pages/WHIMS%20Data.aspx">WHIMS</a></td>
                  </tr>
                  <tr>
                    <th scope="row">4</th>
                    <td>AIBL</td>
                    <td>1088</td>
                    <td>Alzheimer, MCI</td>
                    <td>55 - 96</td>
                    <td><a href="https://aibl.csiro.au/">AIBL</a></td>
                  </tr>
                  <tr>
                    <th scope="row">5</th>
                    <td>OASIS</td>
                    <td>435</td>
                    <td>Alzheimer, MCI</td>
                    <td>18 - 96</td>
                    <td><a href="https://www.oasis-brains.org/">OASIS</a></td>
                  </tr>
                  <tr>
                    <th scope="row">6</th>
                    <td>PPMI</td>
                    <td>610</td>
                    <td>Parkinson</td>
                    <td>30.6 - 84.9</td>
                    <td><a href="https://www.ppmi-info.org/access-data-specimens/download-data/">PPMI</a></td>
                  </tr>
                  <tr>
                    <th scope="row">7</th>
                    <td>MCGill</td>
                    <td>94</td>
                    <td>Parkinson</td>
                    <td>38 - 83</td>
                    <td><a href="http://rpq-qpn.ca/en/">MCGill</a></td>
                  </tr>
                  <tr>
                    <th scope="row">8</th>
                    <td>MCGill</td>
                    <td>94</td>
                    <td>Parkinson</td>
                    <td>38 - 83</td>
                    <td><a href="http://rpq-qpn.ca/en/">MCGill</a></td>
                  </tr>
                  <tr>
                    <th scope="row">9</th>
                    <td>UPenn</td>
                    <td>121</td>
                    <td>Parkinson</td>
                    <td>50 - 84</td>
                    <td>UPenn</a></td>
                  </tr>

                  <tr>
                    <th scope="row">10</th>
                    <td>Liege</td>
                    <td>157</td>
                    <td>Parkinson</td>
                    <td>47 - 87</td>
                    <td>Leige</a></td>
                  </tr>

                  <tr>
                    <th scope="row">11</th>
                    <td>LDM100K</td>
                    <td>100,000</td>
                    <td>Parkinson</td>
                    <td> - </td>
                    <td><a href="https://arxiv.org/abs/2209.07162/">LDM100K</a></td>
                  </tr>

                  <tr>
                    <th scope="row">12</th>
                    <td>NIMHANS</td>
                    <td>323</td>
                    <td> - </td>
                    <td> - </td>
                    <td><a href="https://pubmed.ncbi.nlm.nih.gov/33960571/">NIMHANS</a></td>
                  </tr>

              </tbody> 
            </table>
        </div>
    </div>


    <br>
    <hr class = "container" color="white">

    <br>
    <div class="container text-center">
      <h2 style="color : white"><b>Papers</b></h2>
    </div>
    <br>
    <br>
    <div class = "container blur p-5">
        <div class = "row ">
            <div class="col-md-8">
                <h3 style="color : white; text-align: center;">Improved Brain Age Estimation With Slice-Based Set Networks</h3>
                  <div id = "halfContent1" >
                    <p style="color : white">
                      Deep Learning for neuroimaging data is a promising but challenging direction. The high dimensionality of 3D MRI scans makes this endeavor compute and data-intensive. Most conventional 3D neuroimaging methods use 3D-CNN-based architectures with a large number
                    </p>
                    <div style="text-align: center;">
                      <button class="btn btn-link" onclick="showMore(1)"> Show More <i class="arrow down m-1"></i></button>
                    </div>
                  </div>
                  
                  <div class="d-none" id = "fullContent1">
                    <p style="color : white" >
                      Deep Learning for neuroimaging data is a promising but challenging direction. The high dimensionality of 3D MRI scans makes this endeavor compute and data-intensive. Most conventional 3D neuroimaging methods use 3D-CNN-based architectures with a large number of parameters and 
                      require more time and data to train. Recently, 2D-slice-based models have received increasing attention as they have fewer parameters and may require fewer samples to achieve comparable performance. In this paper, we propose a new architecture for BrainAGE prediction. The proposed architecture works by encoding each 2D slice in an MRI with a deep 2D-CNN model. Next, it combines the information from these 2D-slice encodings using set networks or permutation invariant layers. Experiments on the BrainAGE prediction problem, using the UK Biobank dataset, showed that the model with the permutation invariant layers trains faster and provides better predictions compared to other state-of-the-art approaches.
                    </p>
                    <div style="text-align: center;">
                      <button class="btn btn-link" onclick="showLess(1)"> Show Less <i class="arrow up m-1"></i></button>
                    </div>

                  </div>  
            </div>

            <div class="col-md-4 ">
                <img class="img-fluid" src="./papersImages/ImprovedBrainAgeSliceNetworksUmang.png"></img>
                <br>
                <br>
                <p style="color : grey; text-align: center;" >Published in: 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)  <a href="https://ieeexplore.ieee.org/document/9434081"> Link </a></p>

            </div>
        </div>
    </div>

    <br>
    <br>

    <div class = "container blur p-5">
      <div class = "row ">
          <div class="col-md-8">
              <h3 style="color : white; text-align: center;">Accurate brain age prediction using recurrent slice-based networks
              </h3>
                <div id = "halfContent2" >
                  <p style="color : white">
                    BrainAge (a subject’s apparent age predicted from neuroimaging data) is an important biomarker of brain aging. The deviation of BrainAge from true age has been associated with psychiatric and neurological disease, and has proven effective in predicting conversion from mild cognitive 
                  </p>
                  <div style="text-align: center;">
                    <button class="btn btn-link" onclick="showMore(2)"> Show More <i class="arrow down m-1"></i></button>
                  </div>
                </div>
                
                <div class="d-none" id = "fullContent2">
                  <p style="color : white" >
                    BrainAge (a subject’s apparent age predicted from neuroimaging data) is an important biomarker of brain aging. The deviation of BrainAge from true age has been associated with psychiatric and neurological disease, and has proven effective in predicting conversion from mild cognitive impairment (MCI) to dementia. Conventionally, 3D convolutional neural networks and their variants are used for brain age prediction. However, these networks have a larger number of parameters and take longer to train than their 2D counterparts. Here we propose a 2D slice-based recurrent neural network model, which takes in an ordered sequence of sagittal slices as input to predict the brain age. The model consists of two components: a 2D convolutional neural network (CNN), which encodes the relevant features from the slices, and a recurrent neural network (RNN) that learns the relationship between slices. We compare our method to other recently proposed methods, including 3D deep convolutional regression networks, information theoretic models, and bag-of-features (BoF) models (such as BagNet) - where the classification is based on the occurrences of local features, without taking into consideration their global spatial ordering. In our experiments, our proposed model performs comparably to, or better than, the current state of the art models, with nearly half the number of parameters and a lower convergence time.                  </p>
                  <div style="text-align: center;">
                    <button class="btn btn-link" onclick="showLess(2)"> Show Less <i class="arrow up m-1"></i></button>
                  </div>

                </div>  
          </div>

          <div class="col-md-4 ">
              <img class="img-fluid" src="./papersImages/AccurateBrainAgePradeep.png"></img>
              <br>
              <br>
              <p style="color : grey; text-align: center;" >Read on Biorxiv -  <a href="https://www.biorxiv.org/content/10.1101/2020.08.04.235069v1.full.pdf"> Link </a></p>

          </div>
      </div>
  </div>

  
  <br>
  <br>

  <div class = "container blur p-5">
    <div class = "row ">
        <div class="col-md-8">
            <h3 style="color : white; text-align: center;">Few-Shot Classification of Autism Spectrum Disorder using Site-Agnostic Meta-Learning and Brain MRI
            </h3>
              <div id = "halfContent3" >
                <p style="color : white">
                  For machine learning applications in medical imaging, the availability of training data is often limited, which hampers the design of radiological classifiers for subtle conditions such as autism spectrum disorder (ASD). Transfer learning is one method to counter this problem of low training     </p>
                <div style="text-align: center;">
                  <button class="btn btn-link" onclick="showMore(3)"> Show More <i class="arrow down m-1"></i></button>
                </div>
              </div>
              
              <div class="d-none" id = "fullContent3">
                <p style="color : white" >
                  For machine learning applications in medical imaging, the availability of training data is often limited, which hampers the design of radiological classifiers for subtle conditions such as autism spectrum disorder (ASD). Transfer learning is one method to counter this problem of low training data regimes. Here we explore the use of meta-learning for very low data regimes in the context of having prior data from multiple sites - an approach we term site-agnostic meta-learning. Inspired by the effectiveness of meta-learning for optimizing a model across multiple tasks, here we propose a framework to adapt it to learn across multiple sites. We tested our meta-learning model for classifying ASD versus typically developing controls in 2,201 T1-weighted (T1-w) MRI scans collected from 38 imaging sites as part of Autism Brain Imaging Data Exchange (ABIDE) [age: 5.2-64.0 years]. The method was trained to find a good initialization state for our model that can quickly adapt to data from new unseen sites by fine-tuning on the limited data that is available. The proposed method achieved an ROC-AUC=0.857 on 370 scans from 7 unseen sites in ABIDE using a few-shot setting of 2-way 20-shot i.e., 20 training samples per site. Our results outperformed a transfer learning baseline by generalizing across a wider range of sites as well as other related prior work. We also tested our model in a zero-shot setting on an independent test site without any additional fine-tuning. Our experiments show the promise of the proposed site-agnostic meta-learning framework for challenging neuroimaging tasks involving multi-site heterogeneity with limited availability of training data.
                  <div style="text-align: center;">
                  <button class="btn btn-link" onclick="showLess(3)"> Show Less <i class="arrow up m-1"></i></button>
                </div>

              </div>  
        </div>

        <div class="col-md-4 ">
            <img class="img-fluid" src="./papersImages/FewShotClassificationASDNikhil.png"></img>
            <br>
            <br>
            <p style="color : grey; text-align: center;" >Read on Arxiv -  <a href="https://arxiv.org/abs/2303.08224"> Link </a></p>

        </div>
    </div>
  
  </div>

  <br>
  <br>

  <div class = "container blur p-5">
    <div class = "row ">
        <div class="col-md-8">
            <h3 style="color : white; text-align: center;"> Efficiently Training Vision Transformers on Structural MRI Scans for Alzheimer's Disease Detection </h3>
              <div id = "halfContent4" >
                <p style="color : white">
                  Neuroimaging of large populations is valuable to identify factors that promote or resist brain disease, and to assist diagnosis, subtyping, and prognosis. Data-driven models such as convolutional neural networks (CNNs) have increasingly been applied to brain images to perform<div style="text-align: center;">
                  <button class="btn btn-link" onclick="showMore(4)"> Show More <i class="arrow down m-1"></i></button>
                </div>
              </div>
              
              <div class="d-none" id = "fullContent4">
                <p style="color : white" >
                  Neuroimaging of large populations is valuable to identify factors that promote or resist brain disease, and to assist diagnosis, subtyping, and prognosis. Data-driven models such as convolutional neural networks (CNNs) have increasingly been applied to brain images to perform diagnostic and prognostic tasks by learning robust features. Vision transformers (ViT) - a new class of deep learning architectures - have emerged in recent years as an alternative to CNNs for several computer vision applications. Here we tested variants of the ViT architecture for a range of desired neuroimaging downstream tasks based on difficulty, in this case for sex and Alzheimer's disease (AD) classification based on 3D brain MRI. In our experiments, two vision transformer architecture variants achieved an AUC of 0.987 for sex and 0.892 for AD classification, respectively. We independently evaluated our models on data from two benchmark AD datasets. We achieved a performance boost of 5% and 9-10% upon fine-tuning vision transformer models pre-trained on synthetic (generated by a latent diffusion model) and real MRI scans, respectively. Our main contributions include testing the effects of different ViT training strategies including pre-training, data augmentation and learning rate warm-ups followed by annealing, as pertaining to the neuroimaging domain. These techniques are essential for training ViT-like models for neuroimaging applications where training data is usually limited. We also analyzed the effect of the amount of training data utilized on the test-time performance of the ViT via data-model scaling curves.                  <div style="text-align: center;">
                  <button class="btn btn-link" onclick="showLess(4)"> Show Less <i class="arrow up m-1"></i></button>
                </div>

              </div>  
        </div>

        <div class="col-md-4 ">
            <img class="img-fluid" src="./papersImages/EfficientlyTrainingVITNikhil.png"></img>
            <br>
            <br>
            <p style="color : grey; text-align: center;" >Read on Arxiv -  <a href="https://arxiv.org/ftp/arxiv/papers/2303/2303.08216.pdf"> Link </a></p>

        </div>
    </div>
  
  </div>

  <br><br>
  <div class = "container blur p-5">
    <div class = "row ">
        <div class="col-md-8">
            <h3 style="color : white; text-align: center;"> Evaluation of Transfer Learning Methods for Detecting Alzheimer’s Disease with Brain MRI </h3>
              <div id = "halfContent5" >
                <p style="color : white">
                  Deep neural networks show great promise for classifying brain diseases and making prognostic assessments based on
                  neuroimaging data, but large, labeled training datasets are often required to achieve high predictive accuracy. Here we
                  evaluated a range of transfer learning or pre-training strategies to create useful MRI representations for downstream
                  tasks that lack large amounts of training data, such as Alzheimer’s disease (AD) classification.
                  </p>
                  <div style="text-align: center;">
                    <button class="btn btn-link" onclick="showMore(5)"> Show More <i class="arrow down m-1"></i></button>
                  </div>
              </div>
              
              <div class="d-none" id = "fullContent5">
                <p style="color : white" >
                  Deep neural networks show great promise for classifying brain diseases and making prognostic assessments based on
                  neuroimaging data, but large, labeled training datasets are often required to achieve high predictive accuracy. Here we
                  evaluated a range of transfer learning or pre-training strategies to create useful MRI representations for downstream
                  tasks that lack large amounts of training data, such as Alzheimer’s disease (AD) classification. To test our models, we
                  analyzed 4,098 3D T1-weighted brain MRI scans from the Alzheimer's Disease Neuroimaging Initiative (ADNI) cohort
                  and independently validated our proposed methods for detecting AD with an out-of-distribution test set of 600 scans
                  from the Open Access Series of Imaging Studies (OASIS3) cohort. First, we trained 3D and 2D convolutional neural
                  network (CNN) architectures. We tested combinations of multiple pre-training strategies based on (1) supervised, (2)
                  contrastive learning, and (3) self-supervised learning - using pre-training data within versus outside the MRI domain. In
                  our experiments, the 3D CNN pre-trained with contrastive learning provided the best overall results - when fine-tuned on
                  T1-weighted scans for AD classification - outperformed the baseline by 2.8% when trained with all of the training data
                  from ADNI. We also show test performance as a function of the training dataset size and the chosen pre-training method.
                  Transfer learning offered significant benefits in low data regimes, with a performance boost of 7.7%. When the pretrained model was used for AD classification, we were able to visualize an improved clustering of test subjects'
                  diagnostic groups, as illustrated via a uniform manifold approximation (UMAP) projection of the high-dimensional
                  model embedding space. Further, saliency maps indicate the additional activation regions in the brain scan using pretraining, that then maximally contributed towards the final prediction score
                  </p>
                <div style="text-align: center;">
                  <button class="btn btn-link" onclick="showLess(5)"> Show Less <i class="arrow up m-1"></i></button>
                </div>
              </div>  
        </div>

        <div class="col-md-4 ">
            <img class="img-fluid" src="./papersImages/EvaluationOfTransferLearningNikhil.png"></img>
            <br>
            <br>
            <p style="color : grey; text-align: center;" >Read on Biorxiv -  <a href="https://www.biorxiv.org/content/biorxiv/early/2022/08/25/2022.08.23.505030.full.pdf"> Link </a></p>

        </div>
    </div>
  
  </div>




    <script>
      function showMore(id) {
        document.getElementById("fullContent" + id).classList.remove("d-none");
        document.getElementById("halfContent" + id).classList.add("d-none");

      }
      
      function showLess(id) {
        document.getElementById("fullContent" + id).classList.add("d-none");
        document.getElementById("halfContent" + id).classList.remove("d-none");
      }
    </script>

</body>
</html>
